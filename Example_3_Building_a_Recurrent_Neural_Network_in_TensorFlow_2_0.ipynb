{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example 3-  Building a Recurrent Neural Network in TensorFlow 2.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z82c-Fcay0a3"
      },
      "source": [
        "## Stage 1: Install dependencies and setting up GPU environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoH0-SMEOy-j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "7c4011e9-c2fc-49d0-e119-8b48fb518c8d"
      },
      "source": [
        "!pip install numpy==1.16.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/bf/4981bcbee43934f0adb8f764a1e70ab0ee5a448f6505bd04a87a2fda2a8b/numpy-1.16.1-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 210kB/s \n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement numpy~=1.19.2, but you'll have numpy 1.16.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.19.4\n",
            "    Uninstalling numpy-1.19.4:\n",
            "      Successfully uninstalled numpy-1.19.4\n",
            "Successfully installed numpy-1.16.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL3SBH6PzDwV"
      },
      "source": [
        "## Stage 2: Importing project dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynShOu8nNtFt"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw7-sPdOzf5l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "af708270-ba8d-4204-ea15-f247f0f68765"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEjlM2EazOf0"
      },
      "source": [
        "## Stage 3: Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB0tNtXJzTfA"
      },
      "source": [
        "### Setting up dataset parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw6_KU24SrYK"
      },
      "source": [
        "number_of_words = 20000\n",
        " # this means that we are not taking all reviews from IMBD Dataset, but taking all the reviews that have 20000 most frequently repeated words\n",
        "\n",
        "max_len = 100\n",
        "\n",
        " # for example though we have a review of 5 words a padded version of that review of 5 words is made as sequence of 100 elements of \n",
        "#which 5 elements are of actual review and rest of the elements upto 100 are occupied by padding tokens and all our reviews will be sequences of 100 elements"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePywR8A4zaxT"
      },
      "source": [
        "### Loading the IMDB dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kCTV_hjOKmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d60a97-3405-45aa-a313-ebee93465713"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=number_of_words)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZKDNoTKzi5w"
      },
      "source": [
        "### Padding all sequences to be the same length . To use reviews text data in RNN all the reviews must be of same length. Some reviews can be of 4 or 5 words but some will be 20 words but it is necessary that all these reviews must be of same length , so we add some extra cells in the  tensor of inputs containing all the text so that all the input reviews have the same length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHcMNzv7Pd1s"
      },
      "source": [
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hZ4kfbwKOCP",
        "outputId": "97829ce4-2ebd-42ff-a5d3-285a63520157"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1415,   33,    6, ...,   19,  178,   32],\n",
              "       [ 163,   11, 3215, ...,   16,  145,   95],\n",
              "       [1301,    4, 1873, ...,    7,  129,  113],\n",
              "       ...,\n",
              "       [  11,    6, 4065, ...,    4, 3586,    2],\n",
              "       [ 100, 2198,    8, ...,   12,    9,   23],\n",
              "       [  78, 1099,   17, ...,  204,  131,    9]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcxd--ESP3Rh"
      },
      "source": [
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xDMP44Zz0dU"
      },
      "source": [
        "### Setting up Embedding Layer parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGHQ2upgQIGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3671f2-c9af-40ac-fc56-09471decb847"
      },
      "source": [
        "vocab_size = number_of_words #also known as:  (input_dim = number_of_words)\n",
        "vocab_size"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMyk2JcPQcjF"
      },
      "source": [
        "embed_size = 128 # also known as : no. of columns to encode the words/ represent the words gives embedding matrix of 128 columns or output_dim = 128)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG6LBKGnz7jT"
      },
      "source": [
        "## Step 4: Building a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUVnz-9K0DcW"
      },
      "source": [
        "### Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2GHzwk6OMrV"
      },
      "source": [
        "model = tf.keras.Sequential()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnXJZYR-0HXE"
      },
      "source": [
        "### Adding the Embeding Layer\r\n",
        "\r\n",
        "Embedding layer is the layer used to create a word vector representation of the words( here the words in the reviews ), so that instead of using pre-trained word vectors as if we had vectors of words with padding including, we are going to use this embedded layer to train the vectors in a large matrix. In this large matrix each row corresponds to a word (20,000 words here means 20,000 rows) and coloumns are encoding the words, known as representation of words in Dataset vocabulary. So, by using Embedded layer we are going to learn those word representations jointly with the weights in the network itself. This matrix contains some encoded relationship between the words so that RNN can learn from these relationships and predict in the end if the association of a word leads to a positive or negative review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWqC0DXbO9FU"
      },
      "source": [
        "model.add(tf.keras.layers.Embedding(vocab_size, embed_size, input_shape=(X_train.shape[1],)))\r\n",
        "\r\n",
        " # since shape is the 2nd element of our tensor meaning of index 1 we take shape[1]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM-lpTZX1mEG"
      },
      "source": [
        "### Adding the LSTM Layer ( This layer helps RNN to understand the relationships between the different input words)\n",
        "\n",
        "- units: 128 (No. of cells/neurons in our LSTM Layer)\n",
        "- activation: tanh ( Hyperbolic tangent activation function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W7IXqhjQpAl"
      },
      "source": [
        "model.add(tf.keras.layers.LSTM(units=128, activation='tanh'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T9M5Ult10XM"
      },
      "source": [
        "### Adding the Dense output layer\n",
        "\n",
        "- units: 1 ;\n",
        "\n",
        "( as we have final output as only 0 or 1 where 0 is negative and 1 is positive, we only have one output neuron or cell)\n",
        "\n",
        "- activation: sigmoid \n",
        "\n",
        "(we get the probabilities of reviews if they are positive or negative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe1nHzq7Q91-"
      },
      "source": [
        "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\r\n",
        "\r\n",
        " # since this output layer is fully connected to the previous layer we use dense class"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWcqM4Yr2ALS"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z9ACOXcRUUN"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "#rmsprop is the most recommended optimizer for RNN as it leads to better results"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiolKKO6RjVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b6aa45-4ff9-4959-df72-5c8b9889f1d8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 128)          2560000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,691,713\n",
            "Trainable params: 2,691,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bPUvbfe2GJI"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FqUTA1CRpQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21fb7aed-178b-42cf-cf6f-2c13ce0dc936"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=3, batch_size=128)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "196/196 [==============================] - 12s 23ms/step - loss: 0.5524 - accuracy: 0.7051\n",
            "Epoch 2/3\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.2870 - accuracy: 0.8840\n",
            "Epoch 3/3\n",
            "196/196 [==============================] - 4s 23ms/step - loss: 0.2197 - accuracy: 0.9150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f89924515f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GJ4irh1bCX7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wMo2wYpbCgb"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8kD_6q-RySO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08e1b39-9797-4a36-cb21-0aa49816e938"
      },
      "source": [
        "test_loss, test_acurracy = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 4s 4ms/step - loss: 0.3525 - accuracy: 0.8522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0XnUtS-cEeI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859147bf-96c0-4541-9cb2-6894a37e1aca"
      },
      "source": [
        "print(\"Test accuracy: {}\".format(test_acurracy))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.8521999716758728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN9QK49W3C29"
      },
      "source": [
        "probability_model = tf.keras.Sequential([model,tf.keras.layers.Softmax()])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzUzd-GBJr7s"
      },
      "source": [
        "predictions = probability_model.predict(X_test)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKgqR49qJ17X",
        "outputId": "7d5c4e11-a7af-4de8-dd32-af31bbb6f539"
      },
      "source": [
        "predictions[200]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}